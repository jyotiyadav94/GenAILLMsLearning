{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "RypMxScQk81I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically, it's only a few lines of code to run on GPUs (elsewhere, ie. on Lamini).\n",
        "\n",
        "\n",
        "from llama import BasicModelRunner\n",
        "\n",
        "model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
        "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
        "model.train(is_public=True)\n",
        "\n",
        "Choose base model.\n",
        "Load data.\n",
        "Train it. Returns a model ID, dashboard, and playground interface."
      ],
      "metadata": {
        "id": "4BLP34O9k_tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's look under the hood at the core code running this! This is the open core of Lamini's llama library :)"
      ],
      "metadata": {
        "id": "yS68XFlClEhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install lamini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlL6-E9ElI5n",
        "outputId": "3da50a99-01a6-4d17-b57f-9b3ebf4237ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Collecting lamini\n",
            "  Downloading lamini-1.0.3-47-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m909.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic==1.10.* in /usr/local/lib/python3.10/dist-packages (from lamini) (1.10.13)\n",
            "Collecting lamini-configuration[yaml] (from lamini)\n",
            "  Downloading lamini_configuration-0.8.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lamini) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lamini) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lamini) (1.23.5)\n",
            "Collecting jsonlines (from lamini)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lamini) (1.5.3)\n",
            "Collecting faiss-cpu (from lamini)\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-blob (from lamini)\n",
            "  Downloading azure_storage_blob-12.19.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lamini) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.*->lamini) (4.5.0)\n",
            "Collecting azure-core<2.0.0,>=1.28.0 (from azure-storage-blob->lamini)\n",
            "  Downloading azure_core-1.29.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob->lamini) (41.0.7)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob->lamini)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lamini) (23.1.0)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from lamini-configuration[yaml]->lamini) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lamini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lamini) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (2023.11.17)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lamini) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lamini) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lamini) (3.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob->lamini) (1.16.0)\n",
            "Collecting typing-extensions>=4.2.0 (from pydantic==1.10.*->lamini)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob->lamini) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob->lamini) (2.21)\n",
            "Installing collected packages: faiss-cpu, typing-extensions, lamini-configuration, jsonlines, isodate, azure-core, azure-storage-blob, lamini\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed azure-core-1.29.5 azure-storage-blob-12.19.0 faiss-cpu-1.7.4 isodate-0.6.1 jsonlines-4.0.0 lamini-1.0.3 lamini-configuration-0.8.3 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install data-utilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAZQUskHmfgb",
        "outputId": "6cd5ffb6-d187-4d65-d19e-ef7687875e79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting data-utilities\n",
            "  Downloading data_utilities-1.2.10.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from data-utilities) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from data-utilities) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from data-utilities) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from data-utilities) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from data-utilities) (0.12.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from data-utilities) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->data-utilities) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->data-utilities) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->data-utilities) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->data-utilities) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->data-utilities) (1.16.0)\n",
            "Building wheels for collected packages: data-utilities\n",
            "  Building wheel for data-utilities (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for data-utilities: filename=data_utilities-1.2.10-py3-none-any.whl size=63138 sha256=84a6062d120e5ac34bd62b84216e5b9b06100ef65609cd67a1622282e7130b30\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/f1/50/be14f4ea792e5f2176fe5883831c02bbd71dafec03659d4f80\n",
            "Successfully built data-utilities\n",
            "Installing collected packages: data-utilities\n",
            "Successfully installed data-utilities-1.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utilities-package"
      ],
      "metadata": {
        "id": "-eTjXlQrvjsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjOkZLpGk6xi"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import config\n",
        "import os\n",
        "import yaml\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "\n",
        "#from utilities import *\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "from llama import BasicModelRunner\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Lamini docs dataset"
      ],
      "metadata": {
        "id": "RD7dnTYYlN0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"lamini_docs.jsonl\"\n",
        "dataset_path = f\"/content/{dataset_name}\"\n",
        "use_hf = False"
      ],
      "metadata": {
        "id": "uIEwkAG0lKjw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"lamini/lamini_docs\"\n",
        "use_hf = True"
      ],
      "metadata": {
        "id": "X553mjAQlRBz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the model, training config, and tokenizer"
      ],
      "metadata": {
        "id": "vjZDERnPlT5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/pythia-70m\""
      ],
      "metadata": {
        "id": "qysTBzVflVDo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_config = {\n",
        "    \"model\": {\n",
        "        \"pretrained_name\": model_name,\n",
        "        \"max_length\" : 2048\n",
        "    },\n",
        "    \"datasets\": {\n",
        "        \"use_hf\": use_hf,\n",
        "        \"path\": dataset_path\n",
        "    },\n",
        "    \"verbose\": True\n",
        "}"
      ],
      "metadata": {
        "id": "m74EgKn7lXDO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import tokenize_and_split_data\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "gFEV6lBXlXtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the base model"
      ],
      "metadata": {
        "id": "DUZWizIglaKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "M5HESs5tlbyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    logger.debug(\"Select GPU device\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    logger.debug(\"Select CPU device\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "bNVRhK1Dlcw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.to(device)"
      ],
      "metadata": {
        "id": "iP72m267ld4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to carry out inference"
      ],
      "metadata": {
        "id": "OOknz74mlfr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "OpijF6XllgkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the base model"
      ],
      "metadata": {
        "id": "lp0h6tV9ljP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
        "print(\"Model's answer: \")\n",
        "print(inference(test_text, base_model, tokenizer))"
      ],
      "metadata": {
        "id": "zntNFPWAlkSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup training"
      ],
      "metadata": {
        "id": "W658UaOhlnYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 3"
      ],
      "metadata": {
        "id": "45ES1AAjloXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
        "output_dir = trained_model_name"
      ],
      "metadata": {
        "id": "8xVrpJJ_lp-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=1,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=120, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "id": "T3zSSKGxlsHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_flops = (\n",
        "  base_model.floating_point_ops(\n",
        "    {\n",
        "       \"input_ids\": torch.zeros(\n",
        "           (1, training_config[\"model\"][\"max_length\"])\n",
        "      )\n",
        "    }\n",
        "  )\n",
        "  * training_args.gradient_accumulation_steps\n",
        ")\n",
        "\n",
        "print(base_model)\n",
        "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
        "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
      ],
      "metadata": {
        "id": "0nF8xnTTltTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    model_flops=model_flops,\n",
        "    total_steps=max_steps,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "6inNyHV_luWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a few steps"
      ],
      "metadata": {
        "id": "K1VcCEX7lwLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_output = trainer.train()"
      ],
      "metadata": {
        "id": "a9YMbSz7lxoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save model locally"
      ],
      "metadata": {
        "id": "Q1ePD6kAlyWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ],
      "metadata": {
        "id": "bU45x9pLl03C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
      ],
      "metadata": {
        "id": "hP1MIweil13Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model.to(device)\n"
      ],
      "metadata": {
        "id": "qeAz8imdl20k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run slightly trained model"
      ],
      "metadata": {
        "id": "SPUvpw4Dl4VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_question)\n",
        "\n",
        "print(\"Finetuned slightly model's answer: \")\n",
        "print(inference(test_question, finetuned_slightly_model, tokenizer))"
      ],
      "metadata": {
        "id": "p6EYIMPsl59E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_answer = test_dataset[0]['answer']\n",
        "print(\"Target answer output (test):\", test_answer)"
      ],
      "metadata": {
        "id": "14a481OOl6_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run same model trained for two epochs"
      ],
      "metadata": {
        "id": "2u468mhpl9en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "\n",
        "finetuned_longer_model.to(device)\n",
        "print(\"Finetuned longer model's answer: \")\n",
        "print(inference(test_question, finetuned_longer_model, tokenizer))"
      ],
      "metadata": {
        "id": "mlMz3wG2l-Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run much larger trained model and explore moderation"
      ],
      "metadata": {
        "id": "fppQ0LrwmAsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
        "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
        "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
      ],
      "metadata": {
        "id": "IVxeKtOZmB_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(len(train_dataset)):\n",
        " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
        "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
        "  count += 1\n",
        "print(count)"
      ],
      "metadata": {
        "id": "DqRH5oyWmDnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore moderation using small model"
      ],
      "metadata": {
        "id": "q6cwjA9tmFSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, try the non-finetuned base model:"
      ],
      "metadata": {
        "id": "510U69jxmIl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
      ],
      "metadata": {
        "id": "-1UiyZMHmG9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now try moderation with finetuned small model"
      ],
      "metadata": {
        "id": "yzVN1yFRmL7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
      ],
      "metadata": {
        "id": "Z1AOYditmNP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune a model in 3 lines of code using Lamini"
      ],
      "metadata": {
        "id": "j6nRHyJQmPe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
        "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
        "model.train(is_public=True)"
      ],
      "metadata": {
        "id": "sF7q4wxRmRJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.evaluate()"
      ],
      "metadata": {
        "id": "I5HIbVeImSdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lofd = []\n",
        "for e in out['eval_results']:\n",
        "    q  = f\"{e['input']}\"\n",
        "    at = f\"{e['outputs'][0]['output']}\"\n",
        "    ab = f\"{e['outputs'][1]['output']}\"\n",
        "    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n",
        "    lofd.append(di)\n",
        "df = pd.DataFrame.from_dict(lofd)\n",
        "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
        "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
        "style_df"
      ],
      "metadata": {
        "id": "4EZl_SJfmTeq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}